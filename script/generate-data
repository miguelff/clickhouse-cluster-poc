#!/usr/bin/env ruby
require 'csv'
require 'securerandom'
require 'open3'
require 'fileutils'

class Generator
  attr_reader :projects, :start, :rows, :batch_size, :rps, :debug
  
  def to_s
    "#{self.class.name}: projects=#{projects.size}, start='#{start}', rows=#{rows}, batch_size=#{batch_size}, rps=#{rps}"
  end

  def initialize(args={})
    @start = args[:start] || Time.now
    @rows = args[:rows]
    @batch_size = args[:batch_size]
    @rps = args[:rps]
    @debug = args[:debug]

    output_dir = args[:output_dir]
    unless File.directory?(output_dir)
      FileUtils.mkdir_p(output_dir)
    end

    # caches
    @random_uuids = []
    @random_uuid_sample_set = false
    @random_strings = {}
    @random_string_sample_set = {}

    num_projects = args[:num_projects] || 1
    @projects = [].tap do |p|
      num_projects.times do
        p << SecureRandom.uuid
      end
    end

    @file_name_prefix = File.join(output_dir, "#{table}-rows#{rows}-bsize#{batch_size}_proj#{num_projects}_rps#{rps}")
  end

  def generate(args={})
    1.step(rows, batch_size) do |start_row|
      f = "#{@file_name_prefix}_s#{start_row}.csv"
      if debug
        puts "Generating #{f}"
      end
      CSV.open(f, 'wb') do |csv|
        (start_row...[start_row+batch_size, rows].min).each do |row|
          csv << generate_row(args.merge("row" => row))
        end
      end
    end
  end

  def flush(csv)
    cmd = "docker exec -i #{container} clickhouse-client --query=\"INSERT INTO analytics.#{table} FORMAT CSV\""
    if debug
      puts "DEBUG: flushing #{csv.count} rows: #{cmd}"
    end

    Open3.popen3(cmd) do |i,o,e,t|
      i.write csv.map(&:to_csv).join()
      i.close
      puts o.read
      STDERR.puts e.read
    end

    csv.clear
  end


  def timestamp(start, offset)
    (start + offset).strftime("%Y-%m-%d %H:%M:%S")
  end

  def nullable(prob=0.1)
    return yield if rand > prob
  end

  def random_string(length=8)
    if @random_string_sample_set[length]
      @random_strings[length].sample
    else
      str = random_string_uncached(length)
      @random_strings[length] ||= []
      @random_strings[length] <<
      if @random_strings[length] == 100
         @random_string_sample_set[length] = true
      end
      str
    end
  end

  def random_string_uncached(length=8)
    (0...length).map { (65 + rand(26)).chr }.join
  end

  def random_uuid
    if @random_uuid_sample_set
      @random_uuids.sample
    else
      uuid = SecureRandom.uuid
      @random_uuids << uuid
      if @random_uuids.size == 100
        @random_uuid_sample_set = true
      end
    end
  end

  def self.nil_methods(*args)
    args.each do |a|
      define_method(a) {}
    end
  end
end

class OperationLogsGenerator < Generator

  attr_reader :queries
  
  def to_s
    "#{super}, queries=#{queries.size}"
  end

  def initialize(args={})
    super
    num_queries = args[:num_queries] || 1
    @queries = [].tap do |p|
      num_queries.times do
          p << {hash: random_string_uncached, operation_name: nullable{ random_string_uncached }, operation_type: %w(query mutation).sample, operation_id: nullable{ random_string }, query: nullable { random_string(50)} }
      end
    end
  end

  def table
    "operation_logs"
  end

  def generate_row(args)
    offset = (args["row"] || 0) / (rps|| 10)
    start = Time.now
    query = queries.sample
    [
        projects.sample,
        timestamp(start, offset),
        request_id,
        server_client_id,
        instance_uid,
        client_name,
        query[:operation_type],
        query[:operation_name],
        query[:operation_id],
        transport,
        role,
        query[:query],
        query[:hash],
        session_vars,
        request_size,
        response_size,
        latency,
        request_read_time,
        error,
        error_code,
        http_info,
        websocket_id,
        ws_operation_id,
        opkind,
        generated_sql,
    ]
  end

  nil_methods(:error_code, :http_info, :websocket_id, :ws_operation_id, :opkind, :generated_sql)

  def latency
    rand(1000...1_000_000)
  end
  alias_method :request_read_time, :latency

  def request_size
    nullable { rand(1..1000) }
  end

  def response_size
    nullable { rand(1..1000) }
  end

  def session_vars
    '{}'
  end

  def error
    nullable(0.9) { random_string }
  end

  def client_name
    nullable { random_string }
  end

  def request_id
    random_string(10)
  end

  def server_client_id
    nullable { random_string }
  end

  def transport
    %w(http ws).sample
  end

  def role
    %w(user admin worker).sample
  end

  def parameterized_query_hash
    random_string(40)
  end

  def instance_uid
    random_uuid
  end
end

if $0 == __FILE__
  args = Hash[ARGV.join(' ').scan(/--?([^=\s]+)(?:=(\S+))?/) ]
  args["table"] ||= "operation_logs"
  args["projects"] ||= 10
  args["queries"] ||= 100
  args["rows"] ||= 200_000
  args["batch-size"] ||= 100_000
  args["output-dir"] ||= "/tmp/output"
  args["csv-only"] ||= false
  args["rps"] ||= 1000

  generator = case args["table"]
              when "operation_logs"
                OperationLogsGenerator
              end

  g = generator.new(num_projects: args.delete("projects").to_i,
                    num_queries: args.delete("queries").to_i,
                    rows: args.delete("rows").to_i,
                    batch_size: args.delete("batch-size").to_i,
                    debug: !!args.delete("debug"),
                    rps: args.delete("rps").to_i,
                    output_dir: args.delete("output-dir"))
                    
  if g.debug
    puts g
  end

  g.generate(args)
end


