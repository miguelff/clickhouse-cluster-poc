#!/usr/bin/env ruby
require 'csv'
require 'securerandom'
require 'open3'

class Generator
  attr_reader :projects, :start, :rows, :batch_size, :debug, :container

  def initialize(args={})
    @start = args[:start] || Time.now
    @rows = args[:rows]
    @batch_size = args[:batch_size]
    @debug = args[:debug]
    @container = args[:container]

    num_projects = args[:num_projects] || 1
    @projects = [].tap do |p|
      num_projects.times do
        p << random_uuid
      end
    end
  end

  def generate(args={})
    csv = []
    rows.times do |r|
      csv << generate_row(args.merge("row" => r))
      if (r+1) % batch_size == 0
        flush(csv)
      end
    end
    flush(csv) if csv.length > 0
  end
  
  def flush(csv)
    cmd = "docker exec -i #{container} clickhouse-client --query=\"INSERT INTO analytics.#{table} FORMAT CSV\""
    if debug
      puts "DEBUG: flushing #{csv.count} rows: #{cmd}"
    end

    Open3.popen3(cmd) do |i,o,e,t|
      i.write csv.map(&:to_csv).join()
      i.close
      puts o.read
      STDERR.puts e.read
    end
    
    csv.clear
  end
    

  def timestamp(start, offset)
    (start + offset).strftime("%Y-%m-%d %H:%M:%S")
  end

  def nullable(prob=0.1)
    return yield if rand > prob
  end

  def random_string(length=8)
    (0...length).map { (65 + rand(26)).chr }.join
  end

  def random_uuid
    SecureRandom.uuid
  end

  def self.nil_methods(*args)
    args.each do |a|
      define_method(a) {}
    end
  end
end

class OperationLogsGenerator < Generator

  attr_reader :queries

  def initialize(args={})
    super
    num_queries = args[:num_queries] || 1
    @queries = [].tap do |p|
      num_queries.times do
          p << {hash: random_string, operation_type: nullable{ random_string }, operation_name: %w(query mutation).sample, operation_id: nullable{ random_string }, query: nullable(0.1) { random_string(50)} }
      end
    end
  end
  
  def table
    "operation_logs"
  end

  def generate_row(args)
    offset = (args["row"] || 0) / (args["rps"]&.to_i || 10)
    start = Time.now
    query = queries.sample
    [
        projects.sample,
        timestamp(start, offset),
        request_id,
        server_client_id,
        instance_uid,
        client_name,
        query[:operation_type],
        query[:operation_name],
        query[:operation_id],
        transport,
        role,
        query[:query],
        query[:hash],
        session_vars,
        request_size,
        response_size,
        latency,
        request_read_time,
        error,
        error_code,
        http_info,
        websocket_id,
        ws_operation_id,
        opkind,
        generated_sql,
    ]
  end

  nil_methods(:error_code, :http_info, :websocket_id, :ws_operation_id, :opkind, :generated_sql)

  def latency
    rand(1000...1_000_000)
  end
  alias_method :request_read_time, :latency

  def request_size
    nullable(0.1) { rand(1..1000) }
  end

  def response_size
    nullable(0.1) { rand(1..1000) }
  end

  def session_vars
    '{}'
  end

  def error
    nullable(0.9) { random_string }
  end

  def client_name
    nullable { random_string }
  end

  def request_id
    random_string(10)
  end

  def server_client_id
    nullable { random_string }
  end

  def transport
    %w(http ws).sample
  end

  def role
    %w(user admin worker).sample
  end

  def parameterized_query_hash
    random_string(40)
  end

  def instance_uid
    random_uuid
  end
end

if $0 == __FILE__
  args = Hash[ARGV.join(' ').scan(/--?([^=\s]+)(?:=(\S+))?/) ]
  args["table"] ||= "operation_logs"
  args["projects"] ||= 10
  args["queries"] ||= 100
  args["rows"] ||= 200_000
  args["batch-size"] ||= 100_000
  args["container"] ||= "clickhouse-s1-r1"
  args["csv-only"] ||= false
  args["rps"] ||= 1000

  puts "DEBUG: Generating csv: #{args.inspect}"

  generator = case args["table"]
              when "operation_logs"
                OperationLogsGenerator
              end

  g = generator.new(num_projects: args.delete("projects").to_i, 
                    num_queries: args.delete("queries").to_i,
                    rows: args.delete("rows").to_i, 
                    batch_size: args.delete("batch-size").to_i,
                    debug: !!args.delete("debug"),
                    container: args.delete("container"))
                
  g.generate(args)
end


