#!/usr/bin/env ruby
require 'csv'
require 'securerandom'
require 'open3'
require 'fileutils'
require 'date'

class Generator
  attr_reader :projects, :start, :rows, :batch_size, :rps, :debug

  def to_s
    "#{self.class.name}: projects=#{projects.size}, start='#{start}', rows=#{rows}, batch_size=#{batch_size}, rps=#{rps}"
  end

  def initialize(args={})
    @start = args[:start] || Time.now
    @rows = args[:rows]
    @batch_size = args[:batch_size]
    @rps = args[:rps]
    @debug = args[:debug]

    output_dir = args[:output_dir]
    unless File.directory?(output_dir)
      FileUtils.mkdir_p(output_dir)
    end

    # caches
    @random_strings = {}
    @random_string_sample_set = {}

    num_projects = args[:num_projects] || 1
    @projects = [].tap do |p|
      num_projects.times do
        p << SecureRandom.uuid
      end
    end

    @file_name_prefix = File.join(output_dir, "#{table}-rows#{rows}-bsize#{batch_size}_proj#{num_projects}_rps#{rps}")
  end

  def generate(args={})
    1.step(rows, batch_size) do |start_row|
      f = "#{@file_name_prefix}_s#{start_row}.csv"
      if debug
        puts "Generating #{f}"
      end
      CSV.open(f, 'wb', write_headers: true, headers: headers) do |csv|
        (start_row...[start_row+batch_size, rows].min).each do |row|
          csv << generate_row(args.merge("row" => row))
        end
      end
    end
  end

  def flush(csv)
    cmd = "docker exec -i #{container} clickhouse-client --query=\"INSERT INTO analytics.#{table} FORMAT CSV\""
    if debug
      puts "DEBUG: flushing #{csv.count} rows: #{cmd}"
    end

    Open3.popen3(cmd) do |i,o,e,t|
      i.write csv.map(&:to_csv).join()
      i.close
      puts o.read
      STDERR.puts e.read
    end

    csv.clear
  end


  def timestamp(start, offset)

    (start.to_time + offset).strftime("%Y-%m-%d %H:%M:%S")
  end

  def nullable(prob=0.1)
    return yield if rand > prob
  end

  def more_likely(prob, more_likely)
    if rand < prob
      more_likely
    else
      yield
    end
  end

  def random_string(length=8)
    if @random_string_sample_set[length]
      @random_strings[length].sample
    else
      str = random_string_uncached(length)
      @random_strings[length] ||= []
      @random_strings[length] <<
      if @random_strings[length] == 100
         @random_string_sample_set[length] = true
      end
      str
    end
  end

  def random_string_uncached(length=8)
    (0...length).map { (65 + rand(26)).chr }.join
  end

  def random_uuid
    SecureRandom.uuid
  end

  def self.nil_methods(*args)
    args.each do |a|
      define_method(a) {}
    end
  end
end

class OperationLogsGenerator < Generator

  attr_reader :queries

  def to_s
    "#{super}, queries=#{queries.size}"
  end

  def initialize(args={})
    super
    num_queries = args[:num_queries] || 1
    @queries = [].tap do |p|
      num_queries.times do
          p << {hash: random_string_uncached, operation_name: nullable{ random_string_uncached }, operation_type: %w(query mutation).sample, operation_id: nullable{ random_string }, query: nullable { random_string(50)} }
      end
    end
  end

  def table
    "operation_logs"
  end

  def headers
    %w(project_id timestamp transport group_id instance_uid client_name operation_type operation_name operation_id role request_size response_size latency request_read_time error error_code kind request_mode query parameterized_query_hash session_vars http_info websocket_id)
  end

  def generate_row(args)
    offset = (args["row"] || 0) / (rps|| 10)
    start = self.start
    query = queries.sample

    project_id = projects.sample
    timestamp = self.timestamp(start, offset)
    transport = self.transport
    websocket_id = self.websocket_id if transport == 'ws'
    group_id = self.group_id
    instance_uid = self.instance_uid
    client_name = self.client_name
    operation_type = query[:operation_type]
    operation_name  =  query[:operation_name]
    operation_id = query[:operation_id]
    role = self.role
    request_size = self.request_size
    response_size = self.response_size
    latency = self.latency
    request_read_time = self.request_read_time

    if request_read_time  < latency
      tmp = latency
      latency = request_read_time
      request_read_time = tmp
    end

    error = self.error
    error_code = self.error_code if error
    kind = nil
    request_mode = self.request_mode if transport == 'http'
    query = nil
    parameterized_query_hash = self.parameterized_query_hash
    session_vars = self.session_vars
    http_info = nil

    [
        project_id,
        timestamp,
        transport,
        group_id,
        instance_uid,
        client_name,
        operation_type,
        operation_name,
        operation_id,
        role,
        request_size,
        response_size,
        latency,
        request_read_time,
        error,
        error_code,
        kind,
        request_mode,
        query,
        parameterized_query_hash,
        session_vars,
        http_info,
        websocket_id
    ]
  end

  def latency
    rand(1000...10_000)
  end
  alias_method :request_read_time, :latency

  def request_size
    rand(1..1000)
  end

  def response_size
    rand(1..1000)
  end

  def session_vars
    '{}'
  end

  def error
    nullable(0.9) { random_string }
  end

  def error_code
    rand(400...405)
  end

  def client_name
    nullable { random_string }
  end

  def request_id
    random_string(10)
  end

  def server_client_id
    nullable { random_string }
  end

  def transport
    %w(http ws).sample
  end

  def request_mode
    more_likely(0.8, 'single') { 'batch' }
  end

  def role
    %w(user admin worker).sample
  end

  def parameterized_query_hash
    random_string(40)
  end

  def instance_uid
    random_uuid
  end

  def websocket_id
    random_uuid
  end

  def group_id
    random_uuid
  end
end

if $0 == __FILE__
  args = Hash[ARGV.join(' ').scan(/--?([^=\s]+)(?:=(\S+))?/) ]
  args["table"] ||= "operation_logs"
  args["projects"] ||= 2
  args["queries"] ||= 100
  args["rows"] ||= 1000
  args["batch-size"] ||= args["rows"]
  args["output-dir"] ||= "/tmp/output"
  args["csv-only"] ||= false
  args["rps"] ||= 2
  args["start"] ||= "2040-01-01 09:51:49"

  generator = case args["table"]
              when "operation_logs"
                OperationLogsGenerator
              end

  g = generator.new(num_projects: args.delete("projects").to_i,
                    num_queries: args.delete("queries").to_i,
                    start: DateTime.parse(args["start"]),
                    rows: args.delete("rows").to_i,
                    batch_size: args.delete("batch-size").to_i,
                    debug: !!args.delete("debug"),
                    rps: args.delete("rps").to_i,
                    output_dir: args.delete("output-dir"))

  if g.debug
    puts g
  end

  g.generate(args)
end


